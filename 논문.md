# CNN-Transformer Hybrid Architecture for Image Classification

## 1. Introduction

본 논문에서는 이미지 분류 태스크의 성능 향상을 위해 컨볼루션 신경망(CNN)의 지역적 특징 추출 능력과 트랜스포머의 전역적 관계 모델링 능력을 결합한 하이브리드 아키텍처를 제안한다. 제안하는 모델은 경량 CNN을 패치 단위 특징 추출기(Patch Feature Extractor)로 활용하고, 추출된 특징 시퀀스를 입력으로 받는 크로스-어텐션 기반 트랜스포머 디코더를 통해 최종 분류를 수행한다. 이 구조는 전체 이미지에 대한 셀프-어텐션을 수행하는 Vision Transformer(ViT)와 달리, 소수의 학습 가능한 쿼리(learnable query)를 사용하여 계산 효율성을 높이고 분류에 핵심적인 정보만을 압축적으로 추출한다.

## 2. Proposed Method

제안하는 모델은 크게 세 가지 모듈로 구성된다: (1) Patch-wise CNN Encoder, (2) Cross-Attention Decoder, (3) MLP Classifier.

### 2.1. Patch-wise CNN Encoder

입력 이미지 $I \in \mathbb{R}^{H \times W \times C}$가 주어지면, 먼저 이미지를 겹치지 않는 $N_p$개의 패치(patch)로 분할한다. 각 패치 $p_i \in \mathbb{R}^{S \times S \times C}$ ($i=1, ..., N_p$)는 공유된 가중치를 갖는 CNN 기반 특징 추출기 $\Phi_{cnn}$을 통과한다. 특징 추출기는 사전 훈련된 경량 CNN(e.g., EfficientNet-B0)의 초기 레이어들과 1x1 컨볼루션으로 구성된다. 이 과정을 통해 각 패치는 $D_{feat}$ 차원의 특징 벡터로 변환된다.

$$
\mathbf{x}_i = \text{AdaptiveAvgPool}(\Phi_{cnn}(p_i)) \in \mathbb{R}^{D_{feat}}
$$

모든 패치에 대한 특징 벡터들을 연결하여 인코더는 최종적으로 특징 시퀀스 $X_{enc} = [\mathbf{x}_1, \mathbf{x}_2, ..., \mathbf{x}_{N_p}] \in \mathbb{R}^{N_p \times D_{feat}}$를 생성한다.

### 2.2. Cross-Attention Decoder

디코더는 인코더가 생성한 패치 특징 시퀀스 $X_{enc}$를 보고 분류에 필요한 핵심 정보를 요약하는 역할을 한다. 디코더는 임베딩 모듈과 $L$개의 디코더 레이어로 구성된다.

#### 2.2.1. Input Preparation

디코더는 트랜스포머의 표준 입력인 쿼리(Query), 키(Key), 값(Value)을 준비한다.

*   **Key (K) & Value (V)**: 인코더 출력 시퀀스 $X_{enc}$는 선형 변환 $W_{emb}$와 학습 가능한 위치 인코딩 $PE \in \mathbb{R}^{N_p \times D_{model}}$를 통해 키와 값 시퀀스로 변환된다.

    $$
    K = V = W_{emb}(X_{enc}) + PE \in \mathbb{R}^{N_p \times D_{emb}}
    $$

*   **Query (Q)**: 모델은 $N_q$개의 학습 가능한 파라미터인 `learnable_queries` $Q_{learn} \in \mathbb{R}^{N_q \times D_{feat}}$를 갖는다. 이 쿼리들은 선형 변환을 통해 쿼리 시퀀스 $Q \in \mathbb{R}^{N_q \times D_{emb}}$로 변환된다.
    $$
    Q = W_{emb}(Q_{learn})
    $$
 
    $N_q$는 1과 같은 작은 값으로 설정되어, 모델이 전체 이미지 컨텍스트를 소수의 벡터로 압축하도록 유도한다.

#### 2.2.2. Decoder Layer

각 디코더 레이어는 Multi-Head Cross-Attention (MHCA)과 Feed-Forward Network (FFN) 두 개의 서브-레이어로 구성된다. $l$-번째 레이어의 입력 쿼리를 $Q^{(l-1)}$이라 할 때, 출력 $Q^{(l)}$은 다음과 같이 계산된다.

$$
\hat{Q}^{(l)} = \text{LayerNorm}(Q^{(l-1)} + \text{MHCA}(Q^{(l-1)}, K, V))
$$

$$
Q^{(l)} = \text{LayerNorm}(\hat{Q}^{(l)} + \text{FFN}(\hat{Q}^{(l)}))
$$

여기서 MHCA는 다음과 같이 정의된다.

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

FFN은 두 개의 선형 레이어와 GEGLU 활성화 함수로 구성된다. 이 과정을 $L$번 반복하며 쿼리는 이미지의 분류에 필요한 정보를 점진적으로 정제하고 요약한다.

### 2.3. MLP Classifier

최종 디코더 레이어를 통과한 쿼리 텐서 $Z = Q^{(L)} \in \mathbb{R}^{N_q \times D_{emb}}$는 Flatten된 후 간단한 다층 퍼셉트론(MLP) 분류기 $\Phi_{cls}$에 입력된다. MLP는 최종적으로 $N_{cls}$개의 클래스에 대한 로짓(logit)을 출력한다.
 
$$
\text{Logits} = \Phi_{cls}(\text{Flatten}(Z)) \in \mathbb{R}^{N_{cls}}
$$

## 3. Experimental Pipeline

### 3.1. Dataset and Preprocessing

Sewer-ML 데이터셋을 사용하여 모델을 평가한다. 훈련 데이터에는 `RandomHorizontalFlip`, `ColorJitter` 등의 데이터 증강 기법을 적용하여 모델의 일반화 성능을 높인다. 모든 이미지는 $480 \times 480$ 크기로 리사이즈되며, 사전 훈련된 모델의 통계치로 정규화된다.

### 3.2. Training

모델은 Cross-Entropy Loss를 최소화하도록 훈련된다. 옵티마이저로는 AdamW를 사용하며, 학습률 스케줄러로 Cosine Annealing을 적용한다. 훈련 중 과적합을 방지하기 위해 드롭아웃과 본 연구에서 설계한 Query-Adaptive Masking (QAM)을 트랜스포머 레이어에 적용한다. 검증 데이터셋의 F1 점수를 기준으로 최고 성능 모델을 저장한다.

### 3.3. Inference and Evaluation

훈련이 완료된 후, 저장된 최적의 모델 가중치를 불러와 테스트 데이터셋에 대한 성능을 평가한다. 평가지표로는 정확도(Accuracy), 정밀도(Precision), 재현율(Recall), 그리고 F1 점수를 사용한다. 또한, 모델의 해석 가능성을 위해 마지막 디코더 레이어의 어텐션 가중치를 시각화하여 모델이 이미지의 어느 영역에 집중하는지 분석한다.