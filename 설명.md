**CNN으로 이미지 패치에서 특징을 추출**하고, **트랜스포머(CATS)로 이 특징들을 종합하여 이미지를 분류**하는 하이브리드 모델의 훈련 및 추론 파이프라인입니다.

---

### 1단계: 실행 준비 및 설정 로드 (in `run.py`)

1. **스크립트 실행**: 사용자가 터미널에서 `python run.py --config run.yaml` 명령을 실행하며 모든 과정이 시작됩니다.
2. **설정 파일(`run.yaml`) 파싱**:
    - `main` 함수가 시작되면, `argparse`를 통해 `-config` 인자로 전달된 `run.yaml` 파일의 경로를 얻습니다.
    - `yaml.safe_load` 함수가 이 파일을 읽어 파이썬 딕셔너리로 변환합니다.
    - `SimpleNamespace`를 사용하여, 딕셔너리 키를 `config.run.mode`처럼 점(.)으로 접근할 수 있는 객체(`run_cfg`, `train_cfg`, `model_cfg`, `cats_cfg`)로 만듭니다. 이 객체들은 파이프라인 전반에 걸쳐 하이퍼파라미터를 전달하는 데 사용됩니다.
3. **로깅 및 환경 설정**:
    - `setup_logging` 함수가 호출됩니다. `log/Sewer-ML/` 디렉토리 아래에 `run_20240521_103000`과 같이 현재 시각을 이름으로 갖는 고유한 실행 디렉토리를 생성합니다.
    - 이 디렉토리 안에 모든 실행 결과(로그 파일, 모델 가중치, 그래프 등)가 저장됩니다.
    - `torch.device`를 통해 사용 가능한 GPU(CUDA)를 확인하고, 없으면 CPU를 연산 장치(`device`)로 설정합니다. 이 `device`는 이후 모든 텐서와 모델을 올바른 장치로 보내는 데 사용됩니다.

### 2단계: 데이터 준비 (in `run.py`의 `prepare_data` 함수)

1. **데이터 변환 규칙 정의**:
    - `train_transform`과 `valid_test_transform` 두 가지 데이터 전처리 및 증강 규칙을 정의합니다.
    - **훈련용 (`train_transform`)**: `RandomHorizontalFlip` (좌우 반전), `ColorJitter` (색상 변형) 등 데이터 증강 기법이 포함되어 모델의 일반화 성능을 높입니다.
    - **검증/테스트용 (`valid_test_transform`)**: 데이터 증강 없이 이미지를 `img_size`로 리사이즈하고 텐서로 변환하는 최소한의 처리만 수행하여 일관된 평가 환경을 보장합니다.
    - 두 변환 모두 `Normalize`를 포함하여 이미지 픽셀 값을 특정 평균과 표준편차를 갖도록 정규화합니다.
2. **커스텀 데이터셋 생성**:
    - `CustomImageDataset` 클래스는 `run.yaml`에 명시된 CSV 파일(`train_csv`, `valid_csv`, `test_csv`)과 이미지 디렉토리 경로를 인자로 받습니다.
    - `__init__`에서 `pd.read_csv`로 CSV 파일을 읽어 파일명과 레이블('Defect' 열의 0 또는 1) 정보를 메모리에 로드합니다.
    - `__getitem__` 메소드는 특정 인덱스(idx)가 요청될 때, 해당 이미지 파일을 열고(Pillow 라이브러리 사용), 위에서 정의된 변환(`transform`)을 적용한 뒤, 이미지 텐서와 레이블을 반환합니다.
3. **데이터 샘플링 및 로더 생성**:
    - `run.yaml`의 `random_sampling_ratio` 설정에 따라 `get_subset` 함수가 데이터셋의 일부만 무작위로 샘플링하여 `Subset` 객체를 생성할 수 있습니다. (1.0이면 전체 사용)
    - 최종적으로 `DataLoader`가 `train_dataset`, `valid_dataset`, `test_dataset`을 감싸줍니다. `DataLoader`는 `batch_size` 단위로 데이터를 묶고, 훈련 시에는 데이터를 무작위로 섞어(`shuffle=True`) 모델에 공급하는 역할을 합니다.

### 3단계: 모델 구성 (in `run.py`와 `CATS.py`)

`main` 함수에서 `HybridModel` 클래스를 통해 세 가지 핵심 모듈(`encoder`, `decoder`, `classifier`)을 조립합니다.

### 3.1. 인코더: `PatchConvEncoder` (in `run.py`)

- **역할**: 2D 이미지를 입력받아 여러 개의 작은 조각(패치)으로 나누고, 각 패치에서 특징 벡터를 추출하여 트랜스포머가 처리할 수 있는 **1D 벡터 시퀀스**로 변환합니다.
- **`forward` 상세 흐름**:
    1. **입력**: `[B, C, H, W]` 형태의 이미지 배치 (예: `[64, 3, 480, 480]`)
    2. **패치화 (`unfold`)**: 이미지를 `patch_size` (예: 120x120)에 맞춰 겹치지 않는 패치들로 나눕니다. 결과적으로 `num_encoder_patches` (예: 16개)의 패치가 생성됩니다.
    3. **재배열 (`permute`, `view`)**: 모든 배치의 패치들을 하나의 큰 배치로 만듭니다. (예: `[64 * 16, 3, 120, 120]`)
    4. **특징 추출 (`shared_conv`)**:
        - 이 거대한 패치 배치가 `CnnFeatureExtractor` 모듈로 들어갑니다.
        - `CnnFeatureExtractor`는 `run.yaml`의 `cnn_feature_extractor.name` (예: `efficientnet_b0_feat2`)에 따라 사전 훈련된 CNN 모델의 일부를 로드합니다.
        - 이 CNN이 각 패치를 처리하여 특징 맵(feature map)을 추출합니다.
        - `AdaptiveAvgPool2d((1, 1))`와 `Flatten`을 통해 각 패치의 특징 맵이 `featured_patch_dim` (예: 24) 차원의 벡터 하나로 압축됩니다.
    5. **정규화 및 최종 형태 변환**:
        - `LayerNorm`이 각 패치 특징 벡터를 정규화합니다.
        - 최종적으로 `view`를 통해 텐서의 형태를 `[B, num_encoder_patches, featured_patch_dim]` (예: `[64, 16, 24]`)로 만듭니다. 이것이 인코더의 최종 출력입니다.

### 3.2. 디코더: `CatsDecoder` (실체는 `CATS.py`의 `Model` 클래스)

- **역할**: 인코더가 만든 패치 시퀀스(Key, Value)와, 스스로 학습하는 질문 벡터(Query)를 이용해 **크로스-어텐션**을 수행합니다. 이를 통해 분류에 가장 중요한 정보만을 압축하여 추출합니다.
- **`forward` 상세 흐름**:
    1. **입력**: 인코더의 출력 `[B, 16, 24]`
    2. **임베딩 (`Embedding4Decoder` in `CATS.py`)**:
        - **Key, Value 생성**: 입력된 패치 시퀀스(`[B, 16, 24]`)가 `W_feat2emb` 선형 레이어를 통과하여 모델 내부 차원 `emb_dim` (예: 24)으로 매핑됩니다. 여기에 학습 가능한 `PE`(위치 인코딩)가 더해져 최종 Key와 Value 시퀀스인 `seq_encoder_patches`가 됩니다. (형태: `[B, 16, 24]`)
        - **Query 생성**: `Embedding4Decoder` 내부에 정의된 `learnable_queries` (학습 가능한 파라미터, 예: `[1, 24]`)가 `W_feat2emb`를 통과하고 배치 크기만큼 복제되어 Query 시퀀스인 `seq_decoder_patches`가 됩니다. (형태: `[B, 1, 24]`)
    3. **크로스-어텐션 (`Decoder` in `CATS.py`)**:
        - `Decoder` 모듈은 여러 개의 `DecoderLayer`로 구성됩니다.
        - 각 `DecoderLayer`에서 `_MultiheadAttention`이 호출됩니다. 이때 **Query**(`seq_decoder_patches`)가 **Key/Value**(`seq_encoder_patches`)에게 "이미지 전체 패치들 중에서 현재 분류 작업에 가장 중요한 특징이 무엇인가?"라고 질문을 던집니다.
        - 어텐션 메커니즘은 각 패치의 중요도를 계산(어텐션 가중치)하고, 이를 바탕으로 패치 특징들을 가중합하여 Query를 업데이트합니다.
        - 이 과정이 `num_decoder_layers`만큼 반복되며 정보가 정제됩니다.
    4. **최종 특징 벡터화 (`Projection4Classifier` in `CATS.py`)**:
        - 디코더 레이어를 통과한 최종 Query 텐서(`[B, 1, 24]`)가 `Projection4Classifier`로 전달됩니다.
        - 이 모듈은 텐서를 다시 `featured_patch_dim` 차원으로 변환하고 `Flatten`을 통해 2D 텐서로 만듭니다.
        - **디코더 최종 출력**: `[B, num_decoder_patches * featured_patch_dim]` (예: `[64, 24]`)

### 3.3. 분류기: `Classifier` (in `run.py`)

- **역할**: `CatsDecoder`가 압축한 최종 특징 벡터를 입력받아, 간단한 MLP(다층 퍼셉트론)를 통해 이미지를 최종 클래스('Normal' 또는 'Defect')로 분류합니다.
- **`forward` 상세 흐름**:
    1. **입력**: `CatsDecoder`의 출력 `[B, 24]`
    2. **분류 (`projection`)**: `nn.Sequential`로 구성된 선형 레이어, 활성화 함수(ReLU), 드롭아웃을 거쳐 최종적으로 `num_labels` (2) 차원의 로짓(logit)을 출력합니다.
    3. **최종 모델 출력**: `[B, num_labels]` (예: `[64, 2]`)

### 4단계: 훈련 또는 추론 실행 (in `run.py`)

`run.yaml`의 `mode` 설정에 따라 `train` 또는 `inference` 함수가 호출됩니다.

- **`mode: 'train'` (훈련 모드)**
    1. `train` 함수가 `epochs` 수만큼 훈련 루프를 실행합니다.
    2. 각 에포크마다 `train_loader`에서 미니배치를 가져와 모델을 순전파(`model(images)`)하고, 손실(loss)을 계산(`CrossEntropyLoss`)한 뒤, 역전파(`loss.backward()`)와 가중치 업데이트(`optimizer.step()`)를 수행합니다.
    3. 하나의 에포크 훈련이 끝나면 `evaluate` 함수를 호출하여 `valid_loader`로 검증 성능(F1 점수 등)을 측정합니다.
    4. `best_model_criterion` 설정(예: `F1_Normal`)에 따라 현재 검증 성능이 최고 기록을 경신하면, `torch.save`를 통해 모델의 가중치(`state_dict`)를 `best_model.pth` 파일로 저장합니다.
    5. 모든 에포크가 끝나면, 저장된 최고 성능 모델을 다시 로드하여 `test_loader`로 최종 성능을 평가(`inference` 함수 호출)하고, 정확도 그래프와 혼동 행렬(Confusion Matrix)을 생성하여 실행 디렉토리에 저장합니다.
- **`mode: 'inference'` (추론 모드)**
    1. `run_dir_for_inference` 경로에서 `best_model.pth`를 찾아 `model.load_state_dict`로 가중치를 로드합니다.
    2. 훈련 과정을 건너뛰고 `test_loader`를 사용하여 모델의 성능을 즉시 평가합니다.
    3. `save_attention`이 `True`이면, 어텐션 맵을 시각화하여 이미지 파일로 저장합니다.

이처럼 전체 코드는 **설정 → 데이터 준비 → 모델 조립 → 실행**의 체계적인 파이프라인을 따르며, 각 모듈이 명확한 역할을 수행하도록 잘 설계되어 있습니다.




`run.py`의 `HybridModel` 클래스에서 입력 이미지가 최종 예측 결과로 변환되기까지의 과정을 텐서(Tensor)의 차원 변화와 함께 단계별로 자세히 설명해 드리겠습니다.

설명의 구체성을 위해 `run.yaml` 파일의 주요 설정값을 예시로 사용하겠습니다.

- `batch_size`: 64
- `img_size`: 480
- `in_channels`: 3
- `patch_size`: 120
- `featured_patch_dim`: 24 (CNN이 추출하는 특징 벡터 차원)
- `emb_dim`: 24 (트랜스포머 내부 임베딩 차원)
- `num_decoder_patches`: 1 (학습 가능한 쿼리 개수)
- `num_labels`: 2 (분류할 클래스 수: Normal, Defect)

---

### `HybridModel`의 `forward` 함수 데이터 흐름

`HybridModel`의 `forward` 함수는 세 단계로 구성됩니다: `encoder` → `decoder` → `classifier`.

```python
python

 Show full code block
# c:\Users\Hannibal\Desktop\cau_cats\run.pyclass HybridModel(torch.nn.Module):
    """인코더와 CATS 분류기를 결합한 최종 하이브리드 모델입니다."""
    def __init__(self, encoder, decoder, classifier):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.classifier = classifier

    def forward(self, x):
# 1. 인코딩: 2D 이미지 -> 패치 시퀀스
        x = self.encoder(x)
# 2. 크로스-어텐션: 패치 시퀀스 -> 특징 벡터
        x = self.decoder(x)
# 3. 분류: 특징 벡터 -> 클래스 로짓
        out = self.classifier(x)
        return out

```

### 1단계: `self.encoder(x)` - `PatchConvEncoder`

이 단계의 목표는 2D 이미지를 잘게 쪼개고(패치화), 각 조각에서 특징을 추출하여 트랜스포머가 처리할 수 있는 1D 벡터 시퀀스로 만드는 것입니다.

1. **입력 (Input)**
    - `forward` 함수는 이미지 배치(batch)를 입력으로 받습니다.
    - **텐서 차원**: `[B, C, H, W]` -> `[64, 3, 480, 480]`
2. **패치화 (Patching)**
    - `unfold` 함수가 이미지를 `patch_size`(120x120)에 맞춰 겹치지 않는 조각들로 나눕니다.
    - 가로 4개(480/120), 세로 4개로 총 `4 * 4 = 16`개의 패치가 생성됩니다. (`num_encoder_patches = 16`)
    - `permute`와 `view`를 통해 모든 이미지의 모든 패치를 하나의 큰 배치로 만듭니다.
    - **텐서 차원**: `[B * num_patches, C, patch_size, patch_size]` -> `[1024, 3, 120, 120]`
3. **특징 추출 (Feature Extraction)**
    - `shared_conv` 모듈이 1024개의 각 패치에 대해 CNN(`efficientnet_b0_feat2`)을 적용하여 특징을 추출합니다.
    - `AdaptiveAvgPool2d((1, 1))`를 통해 각 패치의 특징 맵이 `[featured_patch_dim, 1, 1]` 크기로 압축됩니다.
    - `Flatten`을 통해 각 패치가 `featured_patch_dim` 차원의 벡터 하나로 변환됩니다.
    - **텐서 차원**: `[1024, featured_patch_dim]` -> `[1024, 24]`
4. **정규화 및 재구성 (Normalization & Reshaping)**
    - `LayerNorm`이 1024개의 각 패치 특징 벡터를 정규화합니다.
    - `view`를 통해 패치들을 다시 원래 이미지 단위로 묶어 시퀀스 형태로 만듭니다.
    - **Encoder 최종 출력 텐서 차원**: `[B, num_encoder_patches, featured_patch_dim]` -> `[64, 16, 24]`

### 2단계: `self.decoder(x)` - `CatsDecoder` (`CATS.py`의 `Model` 클래스)

이 단계의 목표는 인코더가 만든 16개의 패치 특징 벡터 시퀀스를 보고, 분류에 가장 중요한 정보만을 압축하여 하나의 특징 벡터로 만드는 것입니다.

1. **입력 (Input)**
    - `encoder`의 출력을 입력으로 받습니다.
    - **텐서 차원**: `[64, 16, 24]`
2. **임베딩 및 쿼리/키/값 생성 (`embedding4decoder`)**
    - **Key, Value 생성**: 입력된 패치 시퀀스(`[64, 16, 24]`)가 선형 레이어(`W_feat2emb`)와 위치 인코딩(`PE`)을 거쳐 트랜스포머 내부 차원(`emb_dim`)으로 매핑됩니다. 이것이 어텐션의 **Key**와 **Value**가 됩니다.
        - **텐서 차원**: `[B, num_encoder_patches, emb_dim]` -> `[64, 16, 24]`
    - **Query 생성**: `CatsDecoder`가 스스로 학습하는 `learnable_queries` 파라미터(질문 벡터)를 배치 크기만큼 복제하여 어텐션의 **Query**로 사용합니다.
        - **텐서 차원**: `[B, num_decoder_patches, emb_dim]` -> `[64, 1, 24]`
3. **크로스-어텐션 (`decoder`)**
    - `Query`(`[64, 1, 24]`)가 `Key`/`Value`(`[64, 16, 24]`)에게 "16개 패치 중에서 분류에 가장 중요한 특징이 무엇인가?"라고 질문합니다.
    - 어텐션 메커니즘은 이 질문에 대한 답으로 16개 패치의 특징을 가중합하여 `Query`를 업데이트합니다. 이 과정이 `num_decoder_layers`만큼 반복되며 정보가 정제됩니다.
    - **텐서 차원 (어텐션 후)**: `[B, num_decoder_patches, emb_dim]` -> `[64, 1, 24]`
4. **최종 특징 벡터화 (`projection4classifier`)**
    - 정제된 `Query` 텐서가 `projection4classifier` 모듈을 통과합니다.
    - `Flatten` 레이어가 `num_decoder_patches`와 `featured_patch_dim` 차원을 하나로 합칩니다.
    - **Decoder 최종 출력 텐서 차원**: `[B, num_decoder_patches * featured_patch_dim]` -> `[64, 24]`

### 3단계: `self.classifier(x)` - `Classifier`

이 단계는 `CatsDecoder`가 압축한 최종 특징 벡터를 받아, 이미지가 'Normal'인지 'Defect'인지 최종적으로 분류합니다.

1. **입력 (Input)**
    - `decoder`의 출력을 입력으로 받습니다.
    - **텐서 차원**: `[64, 24]`
2. **분류 (Classification)**
    - 입력 벡터가 간단한 MLP(`projection`)를 통과합니다.
    - MLP는 입력 차원(24)을 받아 최종 클래스 수(`num_labels=2`)에 해당하는 차원으로 매핑합니다. 이 출력값을 로짓(logit)이라고 합니다.
    - **Classifier 최종 출력 텐서 차원**: `[B, num_labels]` -> `[64, 2]`

이 로짓 값에 소프트맥스(Softmax) 함수를 적용하면 각 클래스에 대한 확률을 얻을 수 있으며, 손실 함수(CrossEntropyLoss)는 이 로짓을 직접 사용하여 모델을 학습시킵니다.
