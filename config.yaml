# =============================================================================
# 1. 실행 및 데이터 관련 설정 (run.py, baseline.py 공용)
# =============================================================================
run:
  global_seed: 42 # 프로그램 전체의 재현성을 위한 전역 랜덤 시드. None으로 설정 시 비활성화.
  cuda: true # true: CUDA 사용 시도. false: CPU만 사용. CUDA를 사용할 수 없는 경우 자동으로 CPU로 전환됩니다.
  mode: 'train' # 실행 모드. 'train': 모델 훈련, 'inference': 모델 추론 및 성능 평가
  
  pth_inference_dir: './log/Sewer-TAPNEW/run_20251201_095610'   # 추론 모드에서 사용할 .pth 모델이 포함된 디렉토리의 경로
  pth_best_name: 'best_model.pth' # 훈련 시 저장할 최고 성능 .pth 모델의 파일 이름 또는 추론 시 불러올 모델의 파일 이름.
  evaluate_onnx: true # true: 훈련/추론 후 ONNX로 변환하고 성능을 평가합니다.

# .onnx 파일 경로를 지정하면, inference 모드에서 PyTorch 모델 평가/변환을 건너뛰고 해당 ONNX 모델을 직접 평가합니다.
  # onnx_inference_path: '/home/pi/Desktop/cau_cats/log/Sewer-TAPNEW/baseline_resnet18_20251202_070141/model_20251202_070141.onnx' # onnx_inference_path: './log/Sewer-TAPNEW/run_20251201_095258/model_20251201_095320.onnx' # 평가할 ONNX 파일 경로
  
  only_inference: false # true: 정답 레이블 없이 순수 추론만 수행. false: 정답 레이블과 비교하여 성능 평가.
  show_log: true # true: 로그를 콘솔에 출력하고 파일로 저장. false: 모든 로깅 비활성화.
  
  # --- 데이터셋 설정 ---
  dataset:
    name: 'Sewer-ML' # 사용할 데이터셋 이름 (로깅 및 폴더명 생성에 사용) Sewer-TAPNEW, Sewer-TAP, Sewer-ML
    type: 'csv' # type: 'csv' (이미지 폴더 + CSV 라벨) 또는 'image_folder' (클래스별 폴더 구조)
    train_split_ratio: 0.8 # 'image_folder' 타입일 때 사용할 훈련 데이터 비율 (나머지는 테스트/검증 데이터가 됨)

    # 데이터셋 경로 설정
    paths:
      # 1. 'csv' 타입 데이터셋 경로
      train_img_dir: '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/train'
      valid_img_dir: '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid'
      test_img_dir: '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/valid'
      train_csv: '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Train.csv'
      valid_csv: '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv'
      test_csv: '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-ML/SewerML_Val.csv'

      # 2. 'image_folder' 타입 데이터셋 경로
      # /home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAP
      # /home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW
#       img_folder: '/home/pi/Desktop/tap_new/data'
      # img_folder: '/home/user/workspace/disk/nvme1/data/Sewer/Sewer-TAPNEW'
      # img_folder: '/home/user/workspace/CHOI/Sewer-TAPNEW/data' # 5090서버 임시 데이터
      # img_folder: '/home/cau/workspace/data/Sewer/Sewer-TAPNEW' # 동국대 블랙월 서버

  # 각 데이터셋에서 무작위로 샘플링할 데이터 비율. 1.0이면 전체 데이터 사용.
  random_sampling_ratio:
    train: 1.0
    valid: 1.0
    test: 1.0
  num_workers: 16 # 데이터 로딩 시 사용할 CPU 프로세스 수. 0이면 메인 프로세스만 사용. (시스템 권장값으로 수정)

# =============================================================================
# 2. 훈련 하이퍼파라미터 (run.py 전용)
# =============================================================================
training_run:
  epochs: 90                    # Sewer-ML이 90에포크 사용하여서 동일한 90으로 설정
  batch_size: 256                # 훈련 및 평가 시 사용할 배치 크기. 라즈베리파이 환경에 맞춰 줄임.
  pre_trained: false             # true: ImageNet 사전 훈련 가중치 사용, false: 무작위 초기화 가중치 사용
  optimizer: 'adamw'            # 사용할 옵티마이저. 옵션: 'adamw', 'sgd', 'nadam', 'radam', 'rmsprop'
  lr: 0.001                     # 옵티마이저의 학습률.
  weight_decay: 0.0001             # AdamW 옵티마이저의 가중치 감쇠(Weight Decay) 값.
  loss_function: 'CrossEntropyLoss' # 사용할 손실 함수. 옵션: 'CrossEntropyLoss', 'BCEWithLogitsLoss', 'FocalLoss'
  label_smoothing: 0.1            # 레이블 스무딩(Label Smoothing) 값. 0.0은 비활성화. (과적합 방지, 모델 일반화 성능 향상)
  # --- 학습률 Warmup 설정 ---
  warmup:
    enabled: false               # true: Warmup 사용, false: 사용 안 함
    epochs: 5                   # Warmup을 진행할 에포크 수
    start_lr: 0.00001           # Warmup 시작 시의 학습률 (선형증가)

  # bce_pos_weight: 'auto'        # 'BCEWithLogitsLoss' 사용 시 양성 클래스 가중치. 'auto'로 설정 시 자동 계산, 특정 값(예: 9.0)으로 수동 설정 가능. (num_labels=2여도 코드가 자동으로 [B, 1]로 변환하여 처리)
  # FocalLoss 하이퍼파라미터 (loss_function이 'FocalLoss'일 때 사용)
  # focal_loss_alpha: 0.5          # 클래스 불균형 조절. (0.5: 균등, >0.5: Defect 중요도 상승(Recall 향상), <0.5: Normal 중요도 상승(Precision 향상))
  # focal_loss_gamma: 2.0           # 학습 집중도 조절. (0: 일반 CE Loss, >0: 어려운 샘플에 집중(과적합 완화), 값이 클수록 집중도 상승)
  # 사용할 학습률 스케줄러를 설정합니다.
  scheduler: 'cosineannealinglr' # 옵션: 'multisteplr', 'cosineannealinglr', 'none'
  scheduler_params:
    # CosineAnnealingLR: T_max (주기, 보통 epochs), eta_min (최소 학습률)
    T_max: 90
    eta_min: 0.0001
  best_model_criterion: 'val_loss'  # 최고 모델 저장 기준. 옵션: 'val_loss', 'F1_average', 'F1_Normal', 'F1_Defect'.

# training_run:
#   epochs: 90
#   batch_size: 256
#   pre_trained: false
#   optimizer: 'sgd'
#   loss_function: 'BCEWithLogitsLoss' # num_labels=2여도 코드가 자동으로 [B, 1]로 변환하여 처리
#   bce_pos_weight: 'auto'
#   label_smoothing: 0.1            # 레이블 스무딩(Label Smoothing) 값. 0.0은 비활성화. (과적합 방지, 모델 일반화 성능 향상)
#   lr: 0.1
#   momentum: 0.9
#   weight_decay: 0.0001
#   scheduler: 'multisteplr'
#   milestones: [30, 60, 80]
#   gamma: 0.1
#   best_model_criterion: 'val_loss'  # 최고 모델 저장 기준. 옵션: 'val_loss', 'F1_average', 'F1_Normal', 'F1_Defect'.

# training_run:
#   epochs: 90
#   batch_size: 256
#   pre_trained: false
#   optimizer: 'adamw'
#   loss_function: 'CrossEntropyLoss' # num_labels=2여도 코드가 자동으로 [B, 1]로 변환하여 처리
#   # bce_pos_weight: 'auto'
#   lr: 0.0001
#   # momentum: 0.9
#   weight_decay: 0.01
#   scheduler: 'multisteplr'
#   milestones: [30, 60, 80]
#   gamma: 0.1
#   best_model_criterion: 'val_loss'  # 최고 모델 저장 기준. 옵션: 'val_loss', 'F1_average', 'F1_Normal', 'F1_Defect'.

# =============================================================================
# 3. 훈련 하이퍼파라미터 (baseline.py 전용)
# =============================================================================
training_baseline:
  epochs: 10                    # Sewer-ML이 90에포크 사용하여서 동일한 90으로 설정
  batch_size: 256                # 훈련 및 평가 시 사용할 배치 크기. GPU 메모리에 따라 조절합니다.
  pre_trained: false             # true: ImageNet 사전 훈련 가중치 사용, false: 무작위 초기화 가중치 사용
  optimizer: 'adamw'            # 사용할 옵티마이저. 옵션: 'adamw', 'sgd', 'nadam', 'radam', 'rmsprop'
  lr: 0.001                     # 옵티마이저의 학습률.
  weight_decay: 0.0001             # AdamW 옵티마이저의 가중치 감쇠(Weight Decay) 값.
  loss_function: 'CrossEntropyLoss' # 사용할 손실 함수. 옵션: 'CrossEntropyLoss', 'BCEWithLogitsLoss', 'FocalLoss'
  label_smoothing: 0.1            # 레이블 스무딩(Label Smoothing) 값. 0.0은 비활성화. (과적합 방지, 모델 일반화 성능 향상)
  # --- 학습률 Warmup 설정 ---
  warmup:
    enabled: false               # true: Warmup 사용, false: 사용 안 함
    epochs: 5                   # Warmup을 진행할 에포크 수
    start_lr: 0.00001           # Warmup 시작 시의 학습률 (선형증가)

  scheduler: 'cosineannealinglr' # 옵션: 'multisteplr', 'cosineannealinglr', 'none'
  scheduler_params:
    # CosineAnnealingLR: T_max (주기, 보통 epochs), eta_min (최소 학습률)
    T_max: 10
    eta_min: 0.0001
  best_model_criterion: 'val_loss'  # 최고 모델 저장 기준. 옵션: 'val_loss', 'F1_average', 'F1_Normal', 'F1_Defect'.

# =============================================================================
# 6. Pruning 후 미세 조정(Fine-tuning) 하이퍼파라미터 (baseline.py 전용)
# =============================================================================
finetuning_pruned:
  epochs: 80                     # Pruning 후 성능 복구를 위해 추가 훈련할 에포크 수.
  batch_size: 256                # 미세 조정 시 사용할 배치 크기.
  optimizer: 'adamw'             # 사용할 옵티마이저.
  lr: 0.001                     # 미세 조정 시에는 사전 훈련보다 낮은 학습률을 사용하는 것이 일반적입니다.
  weight_decay: 0.0001           # 가중치 감쇠 값.
  loss_function: 'CrossEntropyLoss' # 손실 함수.
  label_smoothing: 0.1           # 레이블 스무딩 값.
  # --- 학습률 Warmup 설정 ---
  warmup:
    enabled: false
    epochs: 5
    start_lr: 0.000001

  scheduler: 'cosineannealinglr'
  scheduler_params:
    T_max: 80 # fine-tuning epochs에 맞춰 조정
    eta_min: 0.0001
  best_model_criterion: 'val_loss'
# training_baseline:
#   epochs: 90                    # 훈련할 총 에포크 수.
#   batch_size: 256               # 훈련 및 평가 시 사용할 배치 크기.
#   pre_trained: false             # true: ImageNet 사전 훈련 가중치 사용, false: 무작위 초기화 가중치 사용
#   optimizer: 'sgd'              # 사용할 옵티마이저. 옵션: 'adamw', 'sgd'
#   loss_function: 'BCEWithLogitsLoss' # 사용할 손실 함수. 옵션: 'CrossEntropyLoss', 'BCEWithLogitsLoss'. (num_labels=2여도 코드가 자동으로 [B, 1]로 변환하여 처리)
#   bce_pos_weight: 'auto'        # 'BCEWithLogitsLoss' 사용 시 양성 클래스 가중치. 'auto'로 설정 시 자동 계산, 특정 값(예: 9.0)으로 수동 설정 가능.
#   lr: 0.1                       # 옵티마이저의 학습률.
#   momentum: 0.9                 # SGD 옵티마이저의 모멘텀 값.
#   weight_decay: 0.0001          # 옵티마이저의 가중치 감쇠(Weight Decay) 값.
#   scheduler: 'multisteplr'      # 사용할 학습률 스케줄러. 옵션: 'multisteplr', 'cosineannealinglr', 'none'
#   # scheduler_params:
#     # CosineAnnealingLR: T_max (주기, 보통 epochs), eta_min (최소 학습률)
#     # T_max: 90
#     # eta_min: 0.000001
#   milestones: [30, 60, 80]      # MultiStepLR 스케줄러에서 학습률을 감소시킬 에포크 지점.
#   gamma: 0.1                    # MultiStepLR 스케줄러에서 학습률에 곱할 값.
#   best_model_criterion: 'val_loss'  # 최고 모델 저장 기준. 옵션: 'val_loss', 'F1_average', 'F1_Normal', 'F1_Defect'.


# =============================================================================
# 4. 모델 아키텍처 파라미터 (run.py 전용)
# =============================================================================
model:
  # --- 이미지 및 인코더 설정 ---
  img_size: 224                # 모델에 입력하기 전 이미지의 리사이즈 크기 (정사각형).
  patch_size: 56              # 이미지를 나눌 정사각형 패치의 크기. `img_size`는 `patch_size`로 나누어 떨어져야 합니다.
  stride: 56                   # 패치를 추출할 때의 보폭(stride). `patch_size`보다 작으면 오버랩됩니다.
  encoder_type: 'full_image'   # 인코더 방식 선택. 'patch_wise': 패치별 CNN (기본), 'full_image': 전체 이미지 CNN (표준 방식)
  full_image_tokenizer: 'ring_sector'  # 'flatten' 또는 'ring_sector'
  ring_sector:
    num_rings:  4              # 4
    num_sectors:  12             # 12
    mask_outside_circle: false
  cylindrical_token_cnn:
    enabled: true
    theta_kernel_size: 3
    r_kernel_size: 3
    dropout: 0.0
    residual: true
  pool_dim: 2                  # PatchConvEncoder의 풀링 출력 크기 (예: 2 -> 2x2). 기본값 1.
  cnn_feature_extractor:
    # 이미지 패치에서 특징을 추출할 CNN 모델을 선택합니다.
    # 사용 가능한 옵션:
    # - 'resnet18_layer1': ResNet18의 layer1까지 사용. 64
    # - 'resnet18_layer2': ResNet18의 layer2까지 사용. 128

    # - 'mobilenet_v3_small_feat1': MobileNetV3-Small의 features[1]까지 사용. 16
    # - 'mobilenet_v3_small_feat3': MobileNetV3-Small의 features[3]까지 사용. 24
    # - 'mobilenet_v3_small_feat4': MobileNetV3-Small의 features[4]까지 사용. 40

    # - 'efficientnet_b0_feat2': EfficientNet-B0의 features[2]까지 사용 (기본). 24
    # - 'efficientnet_b0_feat3': EfficientNet-B0의 features[3]까지 사용. 40


    # - 'mobilenet_v4_feat1': MobileNetV4-Conv-Small의 features[0]까지 사용. 32
    # - 'mobilenet_v4_feat2': MobileNetV4-Conv-Small의 features[1]까지 사용. 48
    # - 'mobilenet_v4_feat3': MobileNetV4-Conv-Small의 features[2]까지 사용. 64
    # - 'mobilenet_v4_feat4': MobileNetV4-Conv-Small의 features[3]까지 사용. 96
    # - 'mobilenet_v4_feat5': MobileNetV4-Conv-Small의 features[4]까지 사용. 160

    name: 'efficientnet_b0_feat2'    

  # --- 디코더 및 임베딩 설정 ---
  featured_patch_dim: 24     # CNN 특징 추출기가 각 이미지 패치에서 추출하는 특징 벡터의 차원.
  emb_dim: 24                # 트랜스포머 모델 내부에서 사용하는 통일된 임베딩 차원.
  num_heads: 2               # 멀티헤드 어텐션에서 사용할 헤드의 수. `emb_dim`은 `num_heads`로 나누어 떨어져야 합니다.
  num_decoder_layers: 3      # 모델 내부의 디코더 레이어 수.
  num_decoder_patches: 1     # 디코더에서 사용할 학습 가능한 쿼리(패치)의 수. 모델의 표현력을 조절합니다.
  adaptive_initial_query: true # true: 학습 가능한 쿼리와 인코더 출력을 cross-attention하여 동적 초기 쿼리 생성, false: 고정된 학습 가능 쿼리 사용.
  decoder_ff_ratio: 2        # 트랜스포머의 피드포워드 네트워크(FFN) 내부 차원을 결정하는 비율 (decoder_ff_dim = emb_dim * decoder_ff_ratio).
  dropout: 0.1               # 모델 내부에 적용될 드롭아웃 비율 (분류기, FFN, 어텐션 출력에 적용).
  positional_encoding: true  # 이미지 패치 시퀀스 (인코더 출력)에 위치 인코딩 추가 여부. False로 설정하면 위치 정보를 사용하지 않습니다.
  res_attention: true        # 디코더 레이어 간에 어텐션 스코어를 전달하는 잔차 어텐션 사용 여부.
  save_attention: true       # 추론 시 어텐션 맵을 저장할지 여부. True로 설정하면 시각화에 사용할 수 있습니다.
  num_plot_attention: 50     # 어텐션 맵을 시각화하여 저장할 최대 샘플 수.
  pos_encoding_type: ring_sector    # 사용할 PE 타입 ('ring_sector' 또는 'polar' 또는 '2d')
  use_global_context: false   # true: 디코더 초기 쿼리에 전역 컨텍스트 벡터를 주입합니다.

# =============================================================================
# 5. Baseline 모델 설정 (baseline.py 전용)
# =============================================================================
baseline:
  # 사용할 baseline 모델의 이름을 지정합니다.
  # 사용 가능한 옵션: 'resnet18', 'efficientnet_b0', 'mobilenet_v4_s', 'xie2019', 'vit', 'swin_tiny', 'deit_tiny', 'mobile_vit_s', 'mobile_vit_xs', 'mobile_vit_xxs'
  model_name: 'mobile_vit_xxs'

  # --- 경량화 옵션 ---
  use_l1_pruning: false   # L1 Norm Pruning 적용 여부
  use_l2_pruning: false     # L2 Norm Pruning 적용 여부 (torch-pruning 사용)
  use_lamp_pruning: false     # LAMP Pruning (torch-pruning 라이브러리 사용) 적용 여부
  use_slimming_pruning: false # Network Slimming (BN-Scale) Pruning (torch-pruning 라이브러리 사용) 적용 여부
  use_taylor_pruning: false    # Group Taylor Pruning (torch-pruning 사용) 적용 여부
  use_fpgm_pruning: false     # FPGM Pruning (torch-pruning 라이브러리 사용) 적용 여부
  #   pruning_sparsity: 0.812     # 수동으로 설정할 가지치기 희소도. 아래 target_flops 또는 target_params가 0보다 클 경우 이 값은 무시됩니다.
  # pruning_flops_target: 0.1829   # 목표 FLOPs(GFLOPs). 0보다 큰 값으로 설정 시, 이 목표에 가장 가까운 pruning_sparsity를 자동으로 계산합니다.
  pruning_params_target: 0.031371     # 목표 파라미터 수(단위: M). 0보다 큰 값으로 설정 시, 이 목표에 가장 가까운 pruning_sparsity를 자동으로 계산합니다.
