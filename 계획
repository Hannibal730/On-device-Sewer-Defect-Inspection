1차 시도: 바닐라 efficientnet_b0_feat2 85.46
2차: 1차의 qam end 0.5 -> 0.0 (1차 대비 성능 향상. 이후 차시부터 qam=0.0 사용.) 86.23
3차: 2차의 n_heads 4->2 (2차 대비 성능향상. 이후 차시부터 n_heads =2 사용.) 86.38
4차: 3차의 positional_encoding 끄기 (3차 대비 성능 하락. 이후 차시부터 positional encoding true 고정) 85.54
5차: 3차에 스케줄프리 도입 (성능 소폭 상승.) 86.62
6차: 5차의 AdamW 스케줄프리 -> SGD 스케줄프리(모멘텀0.9) 70퍼대 . 30에퐄까지 안 돌려서 기록 없음.
7차: 5차의 efficientnet_b0_feat2 -> mobilenet_v3_small_feat1 80.23
8차: 5차의 efficientnet_b0_feat2 -> resnet18_layer1 84.92
9차: 5차의 데이터 수를 늘리고 에포크도 늘리기. 0.01 -> 0.1, 30 ->50 (9차는 30에포크 이전에 이미 5차보다 성능이 좋았음. 따라서 데이터 개수가 늘수록 성능 향상됨.) 87.18
10차: 5차의 데이터 수를 0.01 -> 0.3, 배치사이즈 8 -> 16.   87.63 (과적합 너무너무 심함. 거의 10에포크 이후로는 최고성능 갱신이 어려워지고 진동도 심함.)

11차: 0.1 데이터 비율로 되돌림.    202850    85.89
배치 사이즈 32
classifier단에 중간층 삽입
디코더 쿼리 패치 개수 계산식을 레이블 개수와 동일시하여 각 쿼리패치가 각 레이블 특징을 학습하도록 유도
qam을 기존에는 패치 순서마다 고정된 선형값을 적용(아마 시계열인 것과 영향..?)이었지만, 각 패치마다 지정범위의 유니폼 분포에서 랜덤값을 마스킹 확률로 사용.
qam 0.1~0.4 

12차: 11차 때 트레인 acc가 늦게 올라가는 게 qam으로 과적합을 성공적으로 방지한 건지 확인하기 위해 qam 0.0~0.0 으로 수정함.  212056   86.71
만약 11차보다 트레인 acc가 들쭉날쭉해진다면 11차에서 qam을 활성화해둔 게 긍정적 효과가 있다는 반증이 된다.
실험결과, epoch당 val acc를 확인해보니까 12차가 11차보다 학습 안정성이 높다. 따라서 qam이 여기서는 효과가 없다고 판단하여 qam 사용 안 하기로 함.


11차, 12차 중 잘 나오는 것에 데이터비율 0.3으로. 
만약 이때 11차 & 0.3 이라면, 10차와 다른 점은 배치 사이즈 16 -> 32, 분류단에 중간층 삽입, qam x -> qam 0.1~0.4
만약 이때 12차 & 0.3 이라면, 10차와 다른 점은 배치 사이즈 16 -> 32, 분류단에 중간층 삽입.

13차: 12차 & 0.3







이러고나면 이제 디코더 레이어 늘리는 것도 시도해봐야 함..



mobilenet_v3_small_feat1 (채널3)
2025-10-19 22:45:34,660 - INFO -   - Encoder (PatchConvEncoder): 1,664 개
2025-10-19 22:45:34,660 - INFO -   - Decoder (CatsDecoder):      10,200 개
2025-10-19 22:45:34,660 - INFO -   - Classifier (Linear Head):   50 개
2025-10-19 22:45:34,660 - INFO -   - 총 파라미터:                  11,914 개





efficientnet_b0_feat2 (채널3)
2025-10-19 04:50:15,348 - INFO -   - Encoder (PatchConvEncoder): 19,138 개
2025-10-19 04:50:15,348 - INFO -   - Decoder (CatsDecoder):      10,200 개
2025-10-19 04:50:15,348 - INFO -   - Classifier (Linear Head):   50 개
2025-10-19 04:50:15,348 - INFO -   - 총 파라미터:                  29,388 개


resnet18_layer1 (3채널)
2025-10-19 23:36:13,126 - INFO -   - Encoder (PatchConvEncoder): 159,112 개
2025-10-19 23:36:13,126 - INFO -   - Decoder (CatsDecoder):      10,200 개
2025-10-19 23:36:13,128 - INFO -   - Classifier (Linear Head):   50 개
2025-10-19 23:36:13,128 - INFO -   - 총 파라미터:                  169,362 개



의 in_channels 3 -> 1



진짜 마지막 실험 때는 피처추출단에 바로 분류단을 연결시켜서 성능을 확인해봐야겠다. 이게 내 꺼보다 성능이 낮아야만 해.



헤드 개수 느는 건 파라미터랑 영향 없음.
아래에 분류단 파라미터 는 건 내가 중간층 삽입했기 때문이다.


디코더 레이어 2
2025-10-21 00:16:52,281 - INFO -   - Encoder (PatchConvEncoder): 19,138 개
2025-10-21 00:16:52,281 - INFO -   - Decoder (CatsDecoder):      10,224 개
2025-10-21 00:16:52,281 - INFO -   - Classifier (Linear Head):   1,277 개
2025-10-21 00:16:52,281 - INFO -   - 총 파라미터:                  30,639 개


디코더 레이어 4
2025-10-21 00:17:57,118 - INFO -   - Encoder (PatchConvEncoder): 19,138 개
2025-10-21 00:17:57,118 - INFO -   - Decoder (CatsDecoder):      18,816 개
2025-10-21 00:17:57,118 - INFO -   - Classifier (Linear Head):   1,277 개
2025-10-21 00:17:57,118 - INFO -   - 총 파라미터:                  39,231 개