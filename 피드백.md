기본 영어문장들을 직접 써보기.
만약 영어문장을 못 쓰겠다면, 이전 논문들에서 가져올 문장들을 찾아서 (구글 스칼라에서 sewer-ml 논문을 )  기본 영어 문장들을 만들고, 거기서부터 시작해야 한다.

구글스칼라에 문장 검색도 되구나.

인용, 빕텍스, 빕에 붙여넣고, 맨 앞에 것을 가져오고, 그걸 





다음주 월요일까지 실험들 마치는 걸로!!




배치 사이즈가 관련되어서 생긴 문재같으니까 이거 감안해서 다시 샘플당 forward pass 시간 테스트

인퍼런스 모드가 그래디언트 안 흐르게 되어 있는지도 체크


TorchScript 또는 torch.compile 사용: PyTorch 모델을 정적 그래프로 변환하여 Python 인터프리터의 오버헤드를 줄이고 연산을 최적화합니다. PyTorch 2.0 이상을 사용한다면 torch.compile이 매우 효과적입니다.

양자화(Quantization): 모델의 가중치를 32비트 부동소수점(FP32)에서 16비트(FP16)나 8비트 정수(INT8)로 변환합니다. 이는 모델 크기를 줄이고, 특히 Tensor Core를 탑재한 NVIDIA GPU에서 추론 속도를 크게 향상시킬 수 있습니다.


## Main Idea

### Encoder: Weight-Shared Patch Feature Extraction
1.  **패치별 특징 추출 (Per-Patch Feature Extraction)**: 입력 이미지를 $N_p$개의 패치로 분할하고, 각 패치 $p_i$를 가중치를 공유하는 CNN($\Phi_{\text{feat}}$)에 통과시켜 특징 벡터 $\mathbf{f}_i$를 추출함. 이 가중치 공유 방식은 모델의 파라미터 효율성을 극대화함.
    $$
    \mathbf{f}_i = \mathrm{Flatten}(\mathrm{AdaptiveAvgPool}(\Phi_{\text{feat}}(p_i))) \in \mathbb{R}^{D_{\text{feat}}}
    $$
2.  **특징 시퀀스 생성 (Feature Sequence Generation)**: 추출된 모든 패치 특징 벡터들을 Layer Normalization(LN) 후 결합하여, 디코더의 Key와 Value로 사용될 최종 특징 시퀀스 $X_{\text{enc}}$를 형성함.
    $$
    X_{\text{enc}} = [\mathrm{LN}(\mathbf{f}_1); \dots; \mathrm{LN}(\mathbf{f}_{N_p})] \in \mathbb{R}^{N_p \times D_{\text{feat}}}
    $$

### Decoder: Input-Adaptive Query with Cross-Attention
1.  **디코더 입력 준비 (Input Preparation)**: 인코더의 최종 출력 $X_{\text{enc}}$에 선형 변환($W_{\text{emb}}$)과 학습 가능한 위치 인코딩($PE$)을 더하여, 모든 디코더 레이어에서 공유될 Key($K$)와 Value($V$)를 준비함.
    $$
    K = V = W_{\text{emb}}(X_{\text{enc}}) + PE \in \mathbb{R}^{N_p \times D_{\text{model}}}
    $$
2.  **동적 쿼리 생성 (Dynamic Query Generation)**: Self-Attention을 배제하고 Cross-Attention만 사용하기 위해, 먼저 입력 이미지의 특징을 요약한 동적 쿼리 $Q_{\text{dynamic}}$를 생성함. 이는 고정된 학습 가능 쿼리($Q_{\text{latent}}$)와 위에서 준비된 이미지 특징맵($K, V$) 간의 사전 어텐션 풀링(Attention Pooling)을 통해 이루어짐.
    $$
    Q_{\text{dynamic}} = \mathrm{softmax}\left(\frac{W_{\text{emb}}(Q_{\text{latent}}) \cdot K^{\top}}{\sqrt{D_{\text{model}}}}\right) \cdot V \in \mathbb{R}^{N_{\text{latent}} \times D_{\text{model}}}
    $$

3.  **디코더 레이어 (Decoder Layers)**: 생성된 동적 쿼리 $Q_{\text{dynamic}}$를 초기 입력($Q^{(0)}$)으로 사용하고, 이미지 특징맵 $K, V$를 모든 레이어에서 공유하며 $L$개의 디코더 레이어를 통과함. 각 레이어는 Multi-Head Cross-Attention(MHCA)과 Feed-Forward Network(FFN)로 구성됨.
    $$
    \hat{Q}^{(l)} = \mathrm{LN}(Q^{(l-1)} + \mathrm{MHCA}(Q^{(l-1)}, K, V))
    $$
    $$
    Q^{(l)} = \mathrm{LN}(\hat{Q}^{(l)} + \mathrm{FFN}(\hat{Q}^{(l)}))
    $$
