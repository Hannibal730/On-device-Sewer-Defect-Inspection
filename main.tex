%% 
%% Copyright 2007-2020 Elsevier Ltd
%% 
%% This file is part of the 'Elsarticle Bundle'.
%% ---------------------------------------------
%% 
%% It may be distributed under the conditions of the LaTeX Project Public
%% License, either version 1.2 of this license or (at your option) any
%% later version.  The latest version of this license is in
%%    http://www.latex-project.org/lppl.txt
%% and version 1.2 or later is part of all distributions of LaTeX
%% version 1999/12/01 or later.
%% 
%% The list of all files belonging to the 'Elsarticle Bundle' is
%% given in the file `manifest.txt'.
%% 
%% Template article for Elsevier's document class `elsarticle'
%% with harvard style bibliographic references

\documentclass[preprint,12pt,authoryear]{elsarticle}

%% Use the option review to obtain double line spacing
%% \documentclass[authoryear,preprint,review,12pt]{elsarticle}

%% Use the options 1p,twocolumn; 3p; 3p,twocolumn; 5p; or 5p,twocolumn
%% for a journal layout:
%% \documentclass[final,1p,times,authoryear]{elsarticle}
%% \documentclass[final,1p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,3p,times,authoryear]{elsarticle}
%% \documentclass[final,3p,times,twocolumn,authoryear]{elsarticle}
%% \documentclass[final,5p,times,authoryear]{elsarticle}
%% \documentclass[final,5p,times,twocolumn,authoryear]{elsarticle}

%% For including figures, graphicx.sty has been loaded in
%% elsarticle.cls. If you prefer to use the old commands
%% please give \usepackage{epsfig}

%% The amssymb package provides various useful mathematical symbols
% \usepackage{amssymb}
% \usepackage[ruled,vlined]{algorithm2e}
\usepackage{graphicx}
\usepackage{algorithmic}
\usepackage{algorithm}
\input{math_commands}
\usepackage{multirow}

\usepackage{pifont}
\newcommand{\xmark}{\ding{55}}%

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
%% The amsthm package provides extended theorem environments
%% \usepackage{amsthm}

%% The lineno packages adds line numbers. Start line numbering with
%% \begin{linenumbers}, end it with \end{linenumbers}. Or switch it on
%% for the whole article with \linenumbers.
%% \usepackage{lineno}

\usepackage[utf8]{inputenc}
\usepackage{verbatim}
\usepackage[USenglish, nodayofweek]{datetime}
\usepackage{url}
\usepackage{graphics}
\usepackage{subfig}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{hyperref}
\usepackage{rotating}
\usepackage{geometry}
\usepackage{pdflscape}
\usepackage{afterpage}
\hypersetup{colorlinks = true, linkcolor = blue, anchorcolor =red, citecolor = blue, filecolor = red, urlcolor = red, pdfauthor=author}

\journal{Engineering Applications of Artificial Intelligence}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%            addressline={}, 
%%            city={},
%%            postcode={}, 
%%            state={},
%%            country={}}
%% \fntext[label3]{}

\title{Low Cost Sewer Defect Diagnosis Transformer for On-Device Processing}
%% use optional labels to link authors explicitly to addresses:
% \author[a]{Dongbin Kim}
% \affiliation[a]{organization={Department of Industrial Engineering, Seoul National University},
%             addressline={Gwanakro 1},
%             city={Seoul},
%             country={Korea}}

% \author[a]{Jaewook Lee}            

% \author[b]{Hoki Kim\corref{cor1}}
% \cortext[cor1]{corresponding author}
% \ead{hokikim@cau.ac.kr}
% \affiliation[b]{organization={Department of Industrial Security, Chung-Ang University},
%             addressline={84, Heukseok-ro},
%             city={Seoul},
%             country={Korea}}

\author{Anonymous}
% \affiliation[a]{organization={Anonymous},
%             addressline={ },
%             city={ },
%             country={ }}


            
\begin{abstract}
%% Text of abstract

% Here is the first paragraph in Introduction,
% “Sewer maintenance is an important issue for the city infrastructure, because undetected sewer defect can cause the urban problems like sinkhole, city flooding and pollution. To prevent these urban problems, sewer experts diagnose sewer system by their eyes, watching the real time images that is given by remote robots. (1)
% However, Dirksen et al. found that human cognitive ability failed to classify the 25% of the sewer defect from German, Netherland, France, Austria sewer datasets. (2)
% For this reason, automated sewer diagnosis system, using deep learning also developed actively to reduce human biases, and to save city infrastructure cost.
% Traditional AI models depends on high performance server and cloud computing resources. (3). But Edge Intelligence deploying AI algorithms to edge-device can reduce the dependency of cloud system reduced and make the local processing more faster. (4)
% However, there are limits for real time processing and automated diagnosis because of its lack of on-device computing resources. (5)

% In this paper, we propose new model to solve this problem.
% New model is a lightweight classification model designed to diagnose the sewer defect on On-device.
% This model divides input images into patches and extracts features from each patch. In this stage, the model shares convolutional weight with all patches to enlarge the efficiency of parameter size.
% And in decoder stage, we use cross -attention mechanism instead of self-attention to reduce the computational cost.
% Also, we set the decoder’s query as few learnable parameters to improve the performance of decoder, by actively adapting the individual input image's features.
% Our models can be the effective solution for the automated sewer defect diagnosis system.” 
% Please correct and revise it.
\begin{comment}
Sewer maintenance is critical to urban infrastructure: undetected defects can trigger sinkholes, flooding, and environmental contamination. Current practice relies on experts who visually inspect real-time video captured by remotely operated robots \citep{HAURUM2020103061}. However, \citet{Dirksen01032013}. reported that human inspectors missed or misclassified about 25\% of defects in datasets from Germany, the Netherlands, France, and Austria. To mitigate such errors and reduce maintenance costs, deep-learning–based automated inspection systems have been actively explored, yet many approaches assume access to high-performance servers or cloud resources \citep{sijingundefined}. Edge intelligence—deploying models on edge devices—reduces reliance on the cloud and enables low-latency local processing \citep{zhou2019edge}. But tight compute and memory budgets still hinder real-time inference and fully automated diagnosis \citep{10.1145/3486618}.

In this paper, we introduce a lightweight on-device classifier for sewer-defect diagnosis. The model partitions each image into patches and applies a shared convolutional encoder across all patches, amortizing parameters and improving efficiency. The decoder replaces self-attention with cross-attention to cut computational cost, and it operates on a small set of learnable queries that adapt to each input image, improving accuracy with minimal overhead. This architecture satisfies edge-device constraints while maintaining strong performance, making it a practical component for automated sewer inspection systems.
\end{comment}
\end{abstract}


%\begin{keyword}
%Rotating machinery \sep Fault diagnosis \sep  On-device\sep Periodicity \sep Patch representation
%\end{keyword}

\end{frontmatter}

% \linenumbers

\section{Introduction}
\label{sec:introduction}

\begin{comment}
Sewer maintenance is very important task for public safety. Because undetected sewer defects can cause urban failures like sinkholes, contamination of groundwater and soil leading public health risks (1).
However, 2025 American Society of Civil Engineers (ASCE) infrastructure report card (2) gave a grade of D+ to the wastewater infrastructure in the United States. 
According to the report, the number of failures per 100 miles of pipe was hovered around 2 since 2017, however it increased to 3.3 in 2021. And it also cited that the cost of upgrading or replacing parts is becoming more expensive.
Therefore, regular inspection of sewage networks is very crucial for urban safety.
\end{comment}

Sewer maintenance is a critical task for ensuring public safety, as undetected sewer defects can lead to urban failures such as sinkholes and contamination of groundwater and soil, which in turn pose serious public health risks \citep{YANG2026107027}. Despite its importance, the infrastructure report card published by the American Society of Civil Engineers assigned a grade of D+ to the wastewater infrastructure in the United States \citep{ASCE2025Infrastructure}. According to this report, the number of failures per 100 miles of pipe hovered around 2 from 2017 onward but increased to 3.3 in 2021, and the cost of upgrading or replacing aging components continues to rise. These trends highlight that regular inspection of sewer networks is crucial for maintaining urban safety.

To diagnose the condition of sewer systems, remotely operated robots are used to record the interior of sewer pipes, and domain experts then inspect the recorded images \citep{NGUYEN2025106479}. This manual inspection process is time-consuming and labor-intensive \citep{WANG2021103840}. Moreover, \citet{Dirksen01032013} reported that human inspectors failed to correctly classify approximately 25\% of sewer defects in datasets from Germany, the Netherlands, France, and Austria. To address these limitations, previous studies have proposed sewer defect detection systems based on conventional machine-learning techniques \citep{doi:10.1061/(ASCE)1076-0342(1999)5:2(69)}. More recently, \citet{KUMAR2018273} proposed a deep learning based sewer defect detection system, and with the rapid progress of deep learning, such methods are now being actively investigated \citep{NASHAT2025295}.

Despite these advances, deep learning–based approaches typically require substantial computational resources, memory, and power to process complex inspection tasks \citep{9985008}. Consequently, deploying sewer defect detection models on edge or embedded devices equipped with low-performance GPUs is challenging \citep{9904106}. To address this limitation, we propose a lightweight sewer defect diagnosis model for on-device deployment that employs a shared-weights cross-attention mechanism.

Our architecture is inspired by CATS \citep{10.5555/3737916.3741543}, which improves model efficiency by removing self-attention, leveraging cross-attention, and sharing parameters










\section{Related Work}
\label{sec:rel}

% perceiver IO처럼 디코더의 쿼리 사이즈를 지정해두고 어텐션하여 모델 사이즈를 줄이는 방식 인용 필요

\subsection{Sewer Defect Diagnosis}
\subsection{Diagnosis Model Architecture}
%  machine learning deep learning
\subsection{On-device System}
.


\section{Methodology}
\label{sec:method}
\subsection{Motivation}

Vision Transformer (ViT) models have recently demonstrated strong performance on a wide range of computer vision tasks. These models typically divide an input image into patches and apply self-attention to model interactions among patch tokens. However, the computational complexity of self-attention scales quadratically with the number of patches. This quadratic cost becomes a major bottleneck when deploying models in on-device environments and embedded autonomous systems with limited computing resources \citep{papa2024survey, li2022efficientformer, mehta2021mobilevit}.

To address this limitation, we propose \textbf{LIQCA}, a \textbf{L}ightweight \textbf{I}nput-conditioned \textbf{Q}uery \textbf{C}ross-\textbf{A}ttention model. LIQCA is designed to be extremely lightweight for resource-constrained devices by replacing self-attention entirely with cross-attention. Our approach is inspired by prior works such as Perceiver, which introduces a small set of latent queries that interact with input tokens via cross-attention, reducing complexity to be linear in the number of input tokens \citep{jaegle2021perceiver}. In addition, using a fixed number of learnable queries to summarize visual features has been shown to improve computational efficiency while retaining essential information \citep{carion2020end}. We adopt these ideas to remove costly self-attention among patch features and enable a small number of learnable queries to summarize global image information efficiently through cross-attention.

LIQCA consists of three main modules: (i) a lightweight CNN-based patch encoder, (ii) a cross-attention-based decoder, and (iii) a final classifier. This design achieves low computational cost and a small parameter count, making it well-suited for on-device deployment.

\subsection{Patch Convolutional Encoder}

The encoder consists of three stages: image patching, feature extraction, and patch mixing. First, the input image is divided into non-overlapping patches. A lightweight CNN feature extractor is then applied to each patch with shared weights to maximize parameter efficiency. The feature map extracted from each patch is compressed into a patch feature using average pooling.

Next, the sequence of patch features is reshaped into a 2D grid, and a depth-wise convolution is applied to integrate local information across neighboring patches. This enables each patch feature to interact with surrounding context at minimal computational cost. This design is motivated by ConvMixer and ConvNeXt, which show that depth-wise convolution can effectively mix spatial context across patches or feature maps \citep{trockman2022patches, liu2022convnet}. The mixed patch features are then flattened back into a 1D sequence as the encoder output, which is used as the keys and values for the decoder.

\subsection{Lightweight Cross-Attention Decoder}

The decoder aims to extract and refine information required for classification from the mixed patch features. Keys and values are obtained by projecting the encoder output to the model embedding dimension and adding positional encoding. We use fixed 2D sinusoidal positional encoding, whose effectiveness has been demonstrated in the Masked Autoencoders (MAE) study \citep{he2022masked}. This encoding provides explicit 2D spatial information without learnable parameters, helping the model capture spatial relationships among patches.

To construct input-conditioned initial queries, we perform cross-attention between a small set of learnable queries and the encoder output. The resulting outputs serve as the initial queries for the decoder, rather than using fixed latent queries. For parameter efficiency, this initialization step employs a single shared projection layer to generate queries, keys, and values. Conceptually, the learnable queries act as questions that adaptively select informative image features during training \citep{jaegle2021perceiver}. By summarizing each input image into a compact set of informative queries before the decoder layers, LIQCA can refine salient features efficiently without requiring additional decoder depth.

The decoder contains $L$ identical layers, each comprising Multi-Head Cross-Attention (MHCA) and a Feed-Forward Network (FFN). Residual connections and Layer Normalization are applied after each sub-layer. Unlike the shared-projection initialization stage, each MHCA layer learns its own independent projection matrices $(W_Q^i, W_K^i, W_V^i)$. The FFN is a two-layer MLP with the GEGLU activation function, which has been shown to improve performance compared to standard ReLU-based FFNs \citep{shazeer2020glu}. The output of the final decoder layer is passed to a lightweight MLP classifier to produce the final class logits.

\begin{algorithm}[H]
\caption{Proposed Method}
\label{alg:main}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\Require Image batch $I \in \mathbb{R}^{B \times C \times H \times W}$, learnable queries $Q_{\text{learn}} \in \mathbb{R}^{N_q \times D_{\text{feat}}}$
\Ensure Class logits $Y \in \mathbb{R}^{B \times N_{\text{cls}}}$

\Statex
\Statex \textbf{1. Patch Convolutional Encoder}
\State Extract patches $\mathcal{P} = {P_1, \dots, P_N}$ from $I$ with kernel $p$ and stride $s$
\State $F_{\text{patch}} \leftarrow \text{AvgPool}(\text{CNN}{\theta}(\mathcal{P})) \in \mathbb{R}^{B \times N \times D{\text{feat}} \times 1 \times 1}$ \Comment{Shared CNN over patches}
\State $F_{\text{grid}} \leftarrow \text{Rearrange}(F_{\text{patch}}) \in \mathbb{R}^{B \times D_{\text{feat}} \times H_p \times W_p}$
\State $F_{\text{mixed}} \leftarrow \text{DWConv}(F_{\text{grid}}) \in \mathbb{R}^{B \times D_{\text{feat}} \times H_p \times W_p}$ \Comment{Local patch mixing}
\State $X_{\text{enc}} \leftarrow \text{LN}(\text{Rearrange}(F_{\text{mixed}})) \in \mathbb{R}^{B \times N \times D_{\text{feat}}}$ \Comment{Encoder output}

\Statex
\Statex \textbf{2. Input-Conditioned Query Initialization}
\State $K, V \leftarrow X_{\text{enc}} W_{\text{emb}} + \text{PE} \in \mathbb{R}^{B \times N \times D_{\text{model}}}$ \Comment{Add 2D sinusoidal PE}
\State $Q_{\text{latent}} \leftarrow Q_{\text{learn}} W_{\text{emb}} \in \mathbb{R}^{N_q \times D_{\text{model}}}$ \Comment{Shared projection layer}
\State $A \leftarrow \text{Softmax}\left( \frac{Q_{\text{latent}} K^\top}{\sqrt{D_{\text{model}}}} \right) \in \mathbb{R}^{B \times N_q \times N}$ \Comment{Attention map}
\State $Q^{(0)} \leftarrow A \cdot V \in \mathbb{R}^{B \times N_q \times D_{\text{model}}}$ \Comment{Input-conditioned initial queries}

\Statex
\Statex \textbf{3. Multi-Head Cross-Attention Decoder}
\For{$l = 1, \dots, L$}
\State $\hat{Q} \leftarrow \text{LN}\left( Q^{(l-1)} + \text{MHCA}(Q^{(l-1)}, K, V) \right) \in \mathbb{R}^{B \times N_q \times D_{\text{model}}}$
\State $Q^{(l)} \leftarrow \text{LN}\left( \hat{Q} + \text{FFN}(\hat{Q}) \right) \in \mathbb{R}^{B \times N_q \times D_{\text{model}}}$
\EndFor

\Statex
\Statex \textbf{4. Classification Head}
\State $Z \leftarrow \text{Linear}(Q^{(L)}) \in \mathbb{R}^{B \times N_q \times D_{\text{feat}}}$
\State $Y \leftarrow \text{MLP}{\text{cls}}(Z) \in \mathbb{R}^{B \times N{\text{cls}}}$
\State \Return $Y$

\end{algorithmic}
\end{algorithm}